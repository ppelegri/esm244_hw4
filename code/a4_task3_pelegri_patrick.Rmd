---
title: 'Task 3: Text analysis (PELEGRI)'
author: "Patrick Pelegri-O'Day"
date: "3/6/2022"
output: html_document
---

```{r setup, include=TRUE, message = FALSE, warning = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)

library(tidyverse)
library(here)
library(pdftools)
library(tidytext)
library(textdata)
library(ggwordcloud)
library(tm)
```

## Overview

This report visualizes the most common words used at the press conference for the release of the most recent International Panel on Climate Change (IPCC) report. In addition, the report compares text sentiment for two speeches given by senior UN officials within the conference: UN Secretary General Antonio Guterres and United Nations Environment Programme director Inge Anderson. The speeches were recorded live on YouTube, and the speech text was obtained from YouTube's transcription feature.

**Citation:** IPCC Press Conference - Climate Change 2022: Impacts, Adaptation, and Vulnerability. Recorded 2022-02-28. https://www.youtube.com/watch?v=JpK7eeYRhjQ

## Setup

Read in data
```{r}
ipcc_text <- pdf_text(here('data', 'ipcc_full.pdf'))
anderson_text <- pdf_text(here('data', 'ipcc_anderson.pdf'))
guterres_text <- pdf_text(here('data', 'ipcc_guterres.pdf'))
```

Initial wrangling
```{r}
anderson_words <- data.frame(anderson_text) %>% 
  mutate(page = 1:n()) %>%
  mutate(text_full = str_split(anderson_text, pattern = '\\n')) %>% 
  unnest(text_full) %>% 
  mutate(text_full = str_trim(text_full)) %>% 
  unnest_tokens(word, text_full) %>% 
  select(-anderson_text)
```

Remove stop words and numbers
```{r}
anderson_words_clean <- anderson_words %>% 
  anti_join(stop_words, by = 'word') %>% 
  filter(word != 'uh', word != 'dr') # removing common words that don't have significant meaning in this context

anderson_words_clean$word <- gsub('[0-9.]', '', anderson_words_clean$word) # remove numeric symbols
```


Count word occurrence
```{r}
anderson_clean_count <- anderson_words_clean %>% 
  count(word)
```


Create word clouds

```{r}
anderson_top50 <- anderson_clean_count %>% 
  arrange(-n) %>% 
  slice(1:50)
```

```{r}
anderson_cloud <- 
  ggplot(data = anderson_top50, aes(label = word)) +
  geom_text_wordcloud(aes(color = n, size = n), shape = "diamond") +
  scale_size_area(max_size = 30) +
  scale_color_gradientn(colors = c("gold1","darkorange2","firebrick4")) +
  theme_minimal()
 
anderson_cloud

```

Sentiment analysis
```{r}
anderson_nrc <- anderson_words_clean %>% 
  inner_join(get_sentiments("nrc"))
```

Let's find the count of words by chapter and sentiment bin: 

```{r}
anderson_nrc_counts <- anderson_nrc %>% 
  count(sentiment)
 
 
ggplot(data = anderson_nrc_counts, aes(x = sentiment, y = n)) +
  geom_col()

```


To-do

* Re-do everything but sentiment with words in all the speeches
* Perform sentiment analysis and visualize
